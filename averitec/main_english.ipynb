{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27910239",
   "metadata": {},
   "source": [
    "Comando para saber em qual GPU o Ollama est√° rodando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e5e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!PID=$(pgrep -f 'ollama run') && nvidia-smi | grep \"$PID\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f015d72",
   "metadata": {},
   "source": [
    "1. AVERITEC - Resultados do Zero Shot (Claim, Questions, Answers) (Gold Evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3772f629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing dev.json\n",
      "üîç Processing train.json\n",
      "\n",
      "üìä Overall Evaluation\n",
      "Claims compared: 3567\n",
      "Exact matches: 1725 (48.4%)\n",
      "\n",
      "üìä Confusion Matrix (True Label √ó Predicted Label):\n",
      "\n",
      "                                                   Supported                  Refuted      Not Enough Evidence     Conflicting Evidence\n",
      "Supported                                                667                       34                       71                      198\n",
      "Refuted                                                  189                      758                      233                      867\n",
      "Not Enough Evidence                                       35                       14                      166                      102\n",
      "Conflicting Evidence/Cherrypickin                         63                       15                       21                      134\n",
      "\n",
      "üìã Classification Report:\n",
      "\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                         Supported      0.699     0.688     0.693       970\n",
      "                           Refuted      0.923     0.370     0.529      2047\n",
      "               Not Enough Evidence      0.338     0.524     0.411       317\n",
      "Conflicting Evidence/Cherrypicking      0.103     0.575     0.175       233\n",
      "\n",
      "                          accuracy                          0.484      3567\n",
      "                         macro avg      0.516     0.539     0.452      3567\n",
      "                      weighted avg      0.757     0.484     0.540      3567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scripts_english/report.py \"./results_english/zero_shot_claim_question_answers_en.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbeee49",
   "metadata": {},
   "source": [
    "2. AVERITEC - Resultados do Zero Shot (Claim, Questions) (No Evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e4daccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing dev.json\n",
      "üîç Processing train.json\n",
      "\n",
      "üìä Overall Evaluation\n",
      "Claims compared: 3567\n",
      "Exact matches: 531 (14.9%)\n",
      "\n",
      "üìä Confusion Matrix (True Label √ó Predicted Label):\n",
      "\n",
      "                                                   Supported                  Refuted      Not Enough Evidence     Conflicting Evidence\n",
      "Supported                                                184                        2                      681                      103\n",
      "Refuted                                                  161                       45                     1355                      486\n",
      "Not Enough Evidence                                       36                        1                      234                       46\n",
      "Conflicting Evidence/Cherrypickin                         25                        0                      140                       68\n",
      "\n",
      "üìã Classification Report:\n",
      "\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                         Supported      0.453     0.190     0.267       970\n",
      "                           Refuted      0.938     0.022     0.043      2047\n",
      "               Not Enough Evidence      0.097     0.738     0.172       317\n",
      "Conflicting Evidence/Cherrypicking      0.097     0.292     0.145       233\n",
      "\n",
      "                          accuracy                          0.149      3567\n",
      "                         macro avg      0.396     0.310     0.157      3567\n",
      "                      weighted avg      0.676     0.149     0.122      3567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scripts_english/report.py \"./results_english/zero_shot_claim_question_en.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e96985",
   "metadata": {},
   "source": [
    "3. AVERITEC - Resultados do Few Shot (Claim, Questions, Answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3feb834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing dev.json\n",
      "üîç Processing train.json\n",
      "\n",
      "üìä Overall Evaluation\n",
      "Claims compared: 3566\n",
      "Exact matches: 1350 (37.9%)\n",
      "\n",
      "üìä Confusion Matrix (True Label √ó Predicted Label):\n",
      "\n",
      "                                                   Supported                  Refuted      Not Enough Evidence     Conflicting Evidence\n",
      "Supported                                                509                       43                       69                      349\n",
      "Refuted                                                  151                      484                      228                     1183\n",
      "Not Enough Evidence                                       12                        3                      175                      127\n",
      "Conflicting Evidence/Cherrypickin                         33                        4                       14                      182\n",
      "\n",
      "üìã Classification Report:\n",
      "\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                         Supported      0.722     0.525     0.608       970\n",
      "                           Refuted      0.906     0.237     0.375      2046\n",
      "               Not Enough Evidence      0.360     0.552     0.436       317\n",
      "Conflicting Evidence/Cherrypicking      0.099     0.781     0.176       233\n",
      "\n",
      "                          accuracy                          0.379      3566\n",
      "                         macro avg      0.522     0.524     0.399      3566\n",
      "                      weighted avg      0.755     0.379     0.431      3566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scripts_english/report.py \"./results_english/few_shot_claim_question_answers_en.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04efcdc2",
   "metadata": {},
   "source": [
    "4. AVERITEC - Resultados do Few Shot (Claim, Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f414f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing dev.json\n",
      "üîç Processing train.json\n",
      "\n",
      "üìä Overall Evaluation\n",
      "Claims compared: 3567\n",
      "Exact matches: 699 (19.6%)\n",
      "\n",
      "üìä Confusion Matrix (True Label √ó Predicted Label):\n",
      "\n",
      "                                                   Supported                  Refuted      Not Enough Evidence     Conflicting Evidence\n",
      "Supported                                                238                       42                      338                      352\n",
      "Refuted                                                  212                      170                      480                     1185\n",
      "Not Enough Evidence                                       36                        4                      120                      157\n",
      "Conflicting Evidence/Cherrypickin                         28                        5                       29                      171\n",
      "\n",
      "üìã Classification Report:\n",
      "\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                         Supported      0.463     0.245     0.321       970\n",
      "                           Refuted      0.769     0.083     0.150      2047\n",
      "               Not Enough Evidence      0.124     0.379     0.187       317\n",
      "Conflicting Evidence/Cherrypicking      0.092     0.734     0.163       233\n",
      "\n",
      "                          accuracy                          0.196      3567\n",
      "                         macro avg      0.362     0.360     0.205      3567\n",
      "                      weighted avg      0.584     0.196     0.201      3567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scripts_english/report.py \"./results_english/few_shot_claim_question_en.json\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
