{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27910239",
   "metadata": {},
   "source": [
    "Comando para saber em qual GPU o Ollama est√° rodando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e5e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!PID=$(pgrep -f 'ollama run') && nvidia-smi | grep \"$PID\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f015d72",
   "metadata": {},
   "source": [
    "1. AVERITEC - Resultados do Zero Shot (Claim, Questions, Answers) (Gold Evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3772f629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processando dev.json\n",
      "üîç Processando train.json\n",
      "\n",
      "üìä Avalia√ß√£o geral\n",
      "Claims comparadas: 3567\n",
      "Acertos exatos: 1505 (42.2%)\n",
      "\n",
      "üìä Matriz de Confus√£o (label verdadeiro √ó classifica√ß√£o do modelo):\n",
      "\n",
      "                                         Supported        Refuted     Not Enough     Conflictin\n",
      "Supported                                      589             96             16            269\n",
      "Refuted                                        185            731             25           1106\n",
      "Not Enough Evidence                             24             14             35            244\n",
      "Conflicting Evidence/Cherrypickin               43             40              0            150\n",
      "\n",
      "üìã Classification Report:\n",
      "\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                         Supported      0.700     0.607     0.650       970\n",
      "                           Refuted      0.830     0.357     0.499      2047\n",
      "               Not Enough Evidence      0.461     0.110     0.178       317\n",
      "Conflicting Evidence/Cherrypicking      0.085     0.644     0.150       233\n",
      "\n",
      "                          accuracy                          0.422      3567\n",
      "                         macro avg      0.519     0.430     0.369      3567\n",
      "                      weighted avg      0.713     0.422     0.489      3567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scripts/report.py \"./results/zero_shot_claim_question_answers.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbeee49",
   "metadata": {},
   "source": [
    "2. AVERITEC - Resultados do Zero Shot (Claim, Questions) (No Evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e4daccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processando dev.json\n",
      "üîç Processando train.json\n",
      "\n",
      "üìä Avalia√ß√£o geral\n",
      "Claims comparadas: 3567\n",
      "Acertos exatos: 508 (14.2%)\n",
      "\n",
      "üìä Matriz de Confus√£o (label verdadeiro √ó classifica√ß√£o do modelo):\n",
      "\n",
      "                                         Supported        Refuted     Not Enough     Conflictin\n",
      "Supported                                      230              6            214            520\n",
      "Refuted                                        212             46            239           1550\n",
      "Not Enough Evidence                             44              2             52            219\n",
      "Conflicting Evidence/Cherrypickin               31              1             21            180\n",
      "\n",
      "üìã Classification Report:\n",
      "\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                         Supported      0.445     0.237     0.309       970\n",
      "                           Refuted      0.836     0.022     0.044      2047\n",
      "               Not Enough Evidence      0.099     0.164     0.123       317\n",
      "Conflicting Evidence/Cherrypicking      0.073     0.773     0.133       233\n",
      "\n",
      "                          accuracy                          0.142      3567\n",
      "                         macro avg      0.363     0.299     0.152      3567\n",
      "                      weighted avg      0.614     0.142     0.129      3567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scripts/report.py \"./results/zero_shot_claim_question.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e96985",
   "metadata": {},
   "source": [
    "3. AVERITEC - Resultados do Few Shot (Claim, Questions, Answers) (Gold Evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3feb834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processando dev.json\n",
      "üîç Processando train.json\n",
      "\n",
      "üìä Avalia√ß√£o geral\n",
      "Claims comparadas: 3567\n",
      "Acertos exatos: 1770 (49.6%)\n",
      "\n",
      "üìä Matriz de Confus√£o (label verdadeiro √ó classifica√ß√£o do modelo):\n",
      "\n",
      "                                         Supported        Refuted     Not Enough     Conflictin\n",
      "Supported                                      482            188             32            268\n",
      "Refuted                                        126           1043             70            808\n",
      "Not Enough Evidence                             18             47            104            148\n",
      "Conflicting Evidence/Cherrypickin               28             57              7            141\n",
      "\n",
      "üìã Classification Report:\n",
      "\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                         Supported      0.737     0.497     0.594       970\n",
      "                           Refuted      0.781     0.510     0.617      2047\n",
      "               Not Enough Evidence      0.488     0.328     0.392       317\n",
      "Conflicting Evidence/Cherrypicking      0.103     0.605     0.176       233\n",
      "\n",
      "                          accuracy                          0.496      3567\n",
      "                         macro avg      0.527     0.485     0.445      3567\n",
      "                      weighted avg      0.699     0.496     0.562      3567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scripts/report.py \"./results/few_shot_claim_question_answers.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04efcdc2",
   "metadata": {},
   "source": [
    "4. AVERITEC - Resultados do Few Shot (Claim, Questions) (No Evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f414f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processando dev.json\n",
      "üîç Processando train.json\n",
      "\n",
      "üìä Avalia√ß√£o geral\n",
      "Claims comparadas: 3567\n",
      "Acertos exatos: 887 (24.9%)\n",
      "\n",
      "üìä Matriz de Confus√£o (label verdadeiro √ó classifica√ß√£o do modelo):\n",
      "\n",
      "                                         Supported        Refuted     Not Enough     Conflictin\n",
      "Supported                                      138            233            270            329\n",
      "Refuted                                         96            507            330           1114\n",
      "Not Enough Evidence                             20             42             89            166\n",
      "Conflicting Evidence/Cherrypickin               12             39             29            153\n",
      "\n",
      "üìã Classification Report:\n",
      "\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                         Supported      0.519     0.142     0.223       970\n",
      "                           Refuted      0.618     0.248     0.354      2047\n",
      "               Not Enough Evidence      0.124     0.281     0.172       317\n",
      "Conflicting Evidence/Cherrypicking      0.087     0.657     0.153       233\n",
      "\n",
      "                          accuracy                          0.249      3567\n",
      "                         macro avg      0.337     0.332     0.226      3567\n",
      "                      weighted avg      0.512     0.249     0.289      3567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scripts/report.py \"./results/few_shot_claim_question.json\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
